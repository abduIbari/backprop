{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Today is about implementing the backpropagation algorithm.\n",
    "We will use the Stochastic Gradient Descent optimizer for optimizing the weights of a custom neural network.\n",
    "\n",
    "You can use numpy or torch for creating tensors, but not for the backpropagation (e.g. loss.backward() )!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def load_mnist_data(root_path=\"./data\", batch_size=4):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.MNIST(root=root_path, train=True, download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    testset = torchvision.datasets.MNIST(root=root_path, train=False, download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Any, Callable, Tuple\n",
    "\n",
    "##################################\n",
    "# For matrices or arbitrary size #\n",
    "##################################\n",
    "class MyWeightTensor:\n",
    "    def __init__(self, shape: Tuple or int, init_weight_fn: Callable = np.random.randn, init_weights: 'MyWeightTensor' or np.ndarray or int or float = None):\n",
    "        assert isinstance(shape, tuple) or isinstance(shape, int) or isinstance(shape, float), f'Allowed shapes: tuple, int, float, got: {type(shape)}'\n",
    "        self.shape = shape\n",
    "\n",
    "        if init_weights is not None:\n",
    "            if isinstance(init_weights, MyWeightTensor):\n",
    "                self.values = init_weights.values\n",
    "            else:\n",
    "                if isinstance(shape, tuple):\n",
    "                    assert isinstance(init_weights, np.ndarray)\n",
    "                else:\n",
    "                    assert isinstance(init_weights, int) or isinstance(init_weights, float)\n",
    "                \n",
    "                self.values = init_weights\n",
    "        else:\n",
    "            if isinstance(shape, int):\n",
    "                self.shape = (self.shape,)\n",
    "                self.values = init_weight_fn(shape)\n",
    "            else:\n",
    "                self.values = init_weight_fn(*shape)\n",
    "    \n",
    "    @property\n",
    "    def T(self) -> 'MyWeightTensor':\n",
    "        _T = self.values.T\n",
    "        return MyWeightTensor(shape=_T.shape, init_weights=_T)\n",
    "    \n",
    "    def __add__(self, other) -> 'MyWeightTensor':\n",
    "        if isinstance(other, MyWeightTensor):\n",
    "            other = other.values\n",
    "        else:\n",
    "            assert isinstance(other, np.ndarray) or isinstance(other, int) or isinstance(other, float)\n",
    "        \n",
    "        return MyWeightTensor(shape=self.values.shape, init_weights=self.values + other)\n",
    "\n",
    "    def __mul__(self, other) -> 'MyWeightTensor':\n",
    "        if isinstance(other, MyWeightTensor):\n",
    "            other = other.values\n",
    "        else:\n",
    "            assert isinstance(other, np.ndarray) or isinstance(other, int) or isinstance(other, float)\n",
    "        \n",
    "        _dot = np.dot(self.values, other)\n",
    "\n",
    "        return MyWeightTensor(shape=_dot.shape, init_weights=_dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For creating a linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearLayer:\n",
    "    def __init__(self, in_features: int, out_features: int, init_weight_fn: Callable = np.random.randn) -> None:\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.weights = MyWeightTensor(shape=(out_features, in_features), init_weight_fn=init_weight_fn)\n",
    "        self.bias = MyWeightTensor(shape=out_features, init_weight_fn=init_weight_fn)\n",
    "\n",
    "        self.latest_input = None\n",
    "        self.latest_output = None\n",
    "\n",
    "    def __call__(self, tensor: np.ndarray or MyWeightTensor) -> MyWeightTensor:\n",
    "        self.latest_input = tensor\n",
    "\n",
    "        bs = -1\n",
    "        if len(tensor.shape) == 2:\n",
    "            # batch size included\n",
    "            bs = tensor.shape[0]\n",
    "            _w = self.weights * tensor.T\n",
    "        else:\n",
    "            _w = self.weights * tensor\n",
    "        \n",
    "        _bias = self.bias.values\n",
    "        if bs != -1:\n",
    "            _bias = np.tile(_bias, bs).reshape(bs, -1)\n",
    "        \n",
    "        self.latest_output = (_w + _bias.T).T\n",
    "\n",
    "        return MyWeightTensor(shape=self.latest_output.shape, init_weights=self.latest_output)\n",
    "    \n",
    "    def derivative(self) -> float:\n",
    "        assert self.latest_output is not None, 'Cannot calculate grad without a single forward pass.'\n",
    "        # Return a linear activation derivation\n",
    "        # your code\n",
    "        return np.ones_like(self.latest_output).astype(float)   # because no activation function is defined in the forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a custom neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_normal_init(*shape) -> np.ndarray:\n",
    "    assert len(shape) <= 2, 'Can only init max 2d tensors'\n",
    "    fan_in = shape[0]\n",
    "    if len(shape) == 1:\n",
    "        fan_out = fan_in\n",
    "    else:\n",
    "        fan_out = shape[1]\n",
    "    gain = 1.0\n",
    "\n",
    "    std = gain * np.sqrt(2.0 / (fan_in + fan_out))\n",
    "    return np.random.normal(loc=0.0, scale=std, size=shape)\n",
    "\n",
    "\n",
    "class MyNeuralNetwork:\n",
    "    def __init__(self) -> None:\n",
    "        # init_weight_fn = lambda *shape: np.random.randn(*shape) / 10\n",
    "        init_weight_fn = lambda *shape: xavier_normal_init(*shape)\n",
    "        \n",
    "        # Build you own neural network with list of MyLinearLayer \n",
    "        # Note numbers of input and final output features of the neural network\n",
    "        # your code\n",
    "\n",
    "        self.layers = [\n",
    "                    MyLinearLayer(784, 25, init_weight_fn),\n",
    "                    MyLinearLayer(25, 25, init_weight_fn),\n",
    "                    MyLinearLayer(25, 10, init_weight_fn)]\n",
    "    \n",
    "    def __call__(self, tensor: np.ndarray) -> Any:\n",
    "        x = tensor\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement your loss function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, predictions: MyWeightTensor or np.ndarray, targets: MyWeightTensor or np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes cross entropy between targets and predictions.    \n",
    "        Returns: List of cross entropy losses (batch-wise)\n",
    "        \"\"\"\n",
    "        if isinstance(predictions, MyWeightTensor):\n",
    "            predictions = predictions.values\n",
    "        \n",
    "        if isinstance(targets, MyWeightTensor):\n",
    "            targets = targets.values\n",
    "\n",
    "        assert predictions.shape[0] == targets.shape[0]\n",
    "        if len(targets.shape) == 2:\n",
    "            targets = targets.reshape(-1)\n",
    "        predictions = torch.as_tensor(predictions)\n",
    "        targets = torch.as_tensor(targets)\n",
    "\n",
    "        loss = np.array([F.cross_entropy(pred, t).item() for pred, t in zip(predictions, targets)])\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def derivative(self) -> Callable:\n",
    "        # y_hat is the prediction\n",
    "        # y is the target value\n",
    "        def _derivative(y_hat: MyWeightTensor or np.ndarray, y: MyWeightTensor or np.ndarray) -> np.ndarray:\n",
    "            if isinstance(y_hat, MyWeightTensor):\n",
    "                y_hat = y_hat.values\n",
    "            \n",
    "            if isinstance(y, MyWeightTensor):\n",
    "                y = y.values\n",
    "\n",
    "            # implement the derivation of the cross entropy \n",
    "            # your code\n",
    "            y_hat_softmax = torch.softmax(torch.tensor(y_hat), dim=1)\n",
    "            return (y_hat_softmax - y).numpy() # This tells us for far our predictions were from the correct labels\n",
    "        \n",
    "        return _derivative   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: MyNeuralNetwork, batch_size: int, learning_rate: float, loss_fn: Callable, epochs: int = 10):\n",
    "    train_loader, _ = load_mnist_data(root_path=\"/Users/abdulbari/Downloads/MNIST\", batch_size=batch_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = []\n",
    "        for imgs, targets in tqdm.tqdm(train_loader, desc=f'Training iteration {epoch + 1}'):\n",
    "\n",
    "            # for custom model\n",
    "            imgs = imgs.numpy()\n",
    "            targets = targets.numpy()\n",
    "\n",
    "            if len(targets.shape) == 1:\n",
    "                targets = targets.reshape(-1, 1)\n",
    "\n",
    "            imgs = imgs.reshape(-1, 28 * 28)\n",
    "\n",
    "            imgs = MyWeightTensor(shape=imgs.shape, init_weights=imgs)\n",
    "\n",
    "            outputs = model(imgs).values\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            avg_loss = np.mean(loss)\n",
    "\n",
    "            # One hot encoding the labels\n",
    "            targets_one_hot = torch.nn.functional.one_hot(torch.tensor(targets), num_classes=10).squeeze(1).float()\n",
    "\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += avg_loss\n",
    "\n",
    "            # Calculate the Accuracy (how many of all samples are correctly classified?)\n",
    "            max_outputs = np.argmax(outputs, axis=1)\n",
    "            accuracy = (max_outputs == targets.flatten()).mean()\n",
    "            running_accuracy.append(accuracy)\n",
    "\n",
    "            #########################\n",
    "            # Start backpropagation #\n",
    "            #########################\n",
    "\n",
    "            # Your code for backpropagation!\n",
    "            \n",
    "            # Step 1: starting from the very end with loss function\n",
    "            # beginning with gradients of the loss\n",
    "\n",
    "            grad_loss = loss_fn.derivative()(outputs, targets_one_hot)\n",
    "\n",
    "            # Step 2: start back propagating the neural network \n",
    "            # for each layer calculate the output gradient and update weights and bias\n",
    "\n",
    "            for layer in reversed(model.layers):\n",
    "                output_grad = grad_loss * (layer.derivative())\n",
    "\n",
    "                dW = (1 / batch_size) * output_grad.T.dot(layer.latest_input.values)\n",
    "                db = (1 / batch_size) * (np.mean(output_grad, axis=0))\n",
    "                grad_loss = grad_loss.dot(layer.weights.values)\n",
    "\n",
    "                layer.weights.values -= learning_rate*(dW)\n",
    "                layer.bias.values -= learning_rate*(db)\n",
    "\n",
    "            #######################\n",
    "            # End backpropagation #\n",
    "            #######################\n",
    "\n",
    "        print(f'Epoch {epoch + 1} finished with loss: {running_loss / len(train_loader):.3f} and accuracy: {torch.tensor(running_accuracy).mean():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 1:   0%|          | 0/15000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 1: 100%|██████████| 15000/15000 [00:08<00:00, 1732.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished with loss: 0.431 and accuracy: 0.871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 2: 100%|██████████| 15000/15000 [00:07<00:00, 1973.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished with loss: 0.354 and accuracy: 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 3: 100%|██████████| 15000/15000 [00:07<00:00, 1986.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished with loss: 0.341 and accuracy: 0.902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 4: 100%|██████████| 15000/15000 [00:07<00:00, 1939.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished with loss: 0.333 and accuracy: 0.904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 5: 100%|██████████| 15000/15000 [00:08<00:00, 1827.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 finished with loss: 0.329 and accuracy: 0.905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 6: 100%|██████████| 15000/15000 [00:07<00:00, 1875.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 finished with loss: 0.325 and accuracy: 0.908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 7: 100%|██████████| 15000/15000 [00:07<00:00, 1882.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 finished with loss: 0.323 and accuracy: 0.907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 8: 100%|██████████| 15000/15000 [00:07<00:00, 1943.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 finished with loss: 0.321 and accuracy: 0.907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 9: 100%|██████████| 15000/15000 [00:09<00:00, 1620.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 finished with loss: 0.319 and accuracy: 0.908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 10: 100%|██████████| 15000/15000 [00:09<00:00, 1658.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 finished with loss: 0.318 and accuracy: 0.909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# Execute the training loop #\n",
    "#############################\n",
    "model = MyNeuralNetwork()\n",
    "batch_size = 4\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    epochs=epochs,\n",
    "    loss_fn=loss_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('aai_2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25301cabe4c6f833fd20f15b1b22933971919908771eb627a83fe325b4fb6671"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
